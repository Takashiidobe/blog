---
title: "Performance Matters"
date: 2020-03-29T13:51:41-05:00
draft: false
---

Asking programmers about performance generally leads to two trains of thought. Either it doesn't matter, and there are other metrics to chase, or performance matters. Most of the time, those who think that performance matters can be broken down into two different streams of thought. One group thinks that performance matters because it saves money -- if your server costs are a million dollars a month and you make the whole system 10% more efficient, you've reduced server costs by about a hundred thousand dollars. It's nice to save that money. The other group is more intense about it -- performance is the only thing that matters because it allows us to create programs that solve problems that other (less efficient) programs would simply not allow us to. Plenty of hot-path code is written in low-level highly optimized languages.

All three ways are true in sometimes: sometimes you don't need to look at code under a microscope because the underlying runtime will optimize it, sometimes saving money may save the company, and sometimes performance is the only thing that matters. But I'm going to say that performance has and always will matter because of the fact that hardware will always get smaller, and we want to do more with less. It turns out that there is a great analogue for this in chemistry already.

In chemistry, atoms are held together by gravitational forces. But there are two forces at work, the `strong` and `weak` force. The `strong force` keeps the nucleus of an atom together, whereas the `weak force` tries to disperse the protons and neutrons of the atom. As you'd expect, the `strong force` is stronger than the `weak force` for smaller elements. But this begins to break down as atoms get bigger -- the `strong force` is not asymptotically stronger than the `weak force`. As such, there are radioactive elements like Uranium-235, where the `weak force` overpowers the `strong force` and causes the element to become radioactive. Radioactive elements eventually tear themselves apart, and split into smaller elements at the end of their half-life.

Hardware has historically (and I don't think this will change much) been the same way. At first, computers like the ENIAC were the size of a house, (wikipedia says 1,800 sq ft), and consumed 150kW of electricity to perform a whopping three square root operations per second. At first, this is wholly insufficient for most of the work needed to be done on a computer. So research was done to optimize hardware to the point where it was able to be made smaller. That led to the micro-computer, a computer the size of a desk, which could do some interesting tasks like word processing or simple text games. Eventually we found that we had enough performance that we could stick a micro-computer into a smaller computer, one that was portable, and that was called a laptop. Then the hardware advanced to the point where it could fit into your pocket, and that became the smart phone. Tablets branched out to be in the middle of size (and therefore compute) of a laptop and a smart phone. Our need for smaller and more efficient hardware has made it so we couldn't throw performance by the wayside; every time performance was "good enough", the hardware got smaller until performance mattered again.

If you believe history will continue, then we'll have even smaller devices with even more strict real time capabilities. Doctors have been asking for more efficient tools to treat patients with for a very long time, for good reason. More portable and cheaper tools for doctors allow them to treat more patients at less cost. More powerful tools allow for doctors to find signs that they might've missed with less powerful tools. A new generation of video gamers want to bring their games on the go -- as such, the mobile video game industry has blossomed apart from the traditional console based games. VR headsets are now being used for therapy, and this is extremely performance sensitive -- if the program on a VR headset is slow by even milliseconds, the user may become fatigued or nauseous, which defeats the purpose of the technology.

History has always been about doing more with less, and I don't see that changing for the near future. Maybe when Moore's law stops becoming true?
